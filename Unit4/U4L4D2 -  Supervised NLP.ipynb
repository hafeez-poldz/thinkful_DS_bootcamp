{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\hafeez_poldz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\hafeez_poldz\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "symbolic link created for C:\\Users\\hafeez_poldz\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\hafeez_poldz\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Linking successful\n",
      "C:\\Users\\hafeez_poldz\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\hafeez_poldz\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.\n",
    "\n",
    "**Note**: Since processing all the text takes around ~5-10 minutes, in the cell below we are taking only the first tenth of each text. If you want to experiment, feel free to change the following code in the next cell:\n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n",
    "```\n",
    "to \n",
    "\n",
    "```python\n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load(\"en\")\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observance</th>\n",
       "      <th>mercy</th>\n",
       "      <th>faculty</th>\n",
       "      <th>poor</th>\n",
       "      <th>dip</th>\n",
       "      <th>claim</th>\n",
       "      <th>rejoin</th>\n",
       "      <th>agreeable</th>\n",
       "      <th>believe</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>expedient</th>\n",
       "      <th>hope</th>\n",
       "      <th>issue</th>\n",
       "      <th>think</th>\n",
       "      <th>deal</th>\n",
       "      <th>consideration</th>\n",
       "      <th>advice</th>\n",
       "      <th>thing</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  observance mercy faculty poor dip claim rejoin agreeable believe  9  ...  \\\n",
       "0          0     0       0    0   0     0      0         0       0  0  ...   \n",
       "1          0     0       0    0   0     0      0         0       0  0  ...   \n",
       "2          0     0       0    0   0     0      0         0       0  0  ...   \n",
       "3          0     0       0    0   0     0      0         0       0  0  ...   \n",
       "4          0     0       0    0   0     0      0         0       0  0  ...   \n",
       "\n",
       "  expedient hope issue think deal consideration advice thing  \\\n",
       "0         0    0     0     1    0             0      0     0   \n",
       "1         0    0     0     0    0             0      0     0   \n",
       "2         0    0     0     1    0             0      0     0   \n",
       "3         0    0     0     0    0             0      0     0   \n",
       "4         0    0     0     0    0             0      0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 1615 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.981203007518797\n",
      "\n",
      "Test set score: 0.8539325842696629\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9699248120300752\n",
      "\n",
      "Test set score: 0.8764044943820225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9661654135338346\n",
      "\n",
      "Test set score: 0.8089887640449438\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data. Remember that for computation time concerns, we only took the first tenth of the Alice text. Emma is pretty long. **So in order to get comparable length texts, we take the first sixtieth of Emma**. Again, if you want to experiment, you can take the whole texts of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma[:int(len(emma)/60)])\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.,\n",
       "  'Austen'],\n",
       " [She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.,\n",
       "  'Austen'],\n",
       " [Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.,\n",
       "  'Austen'],\n",
       " [Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of Emma.,\n",
       "  'Austen'],\n",
       " [Between _them, 'Austen'],\n",
       " [_, 'Austen'],\n",
       " [it was more the intimacy of sisters., 'Austen'],\n",
       " [Even before Miss Taylor had ceased to hold the nominal office of governess, the mildness of her temper had hardly allowed her to impose any restraint; and the shadow of authority being now long passed away, they had been living together as friend and friend very mutually attached, and Emma doing just what she liked; highly esteeming Miss Taylor's judgment, but directed chiefly by her own.,\n",
       "  'Austen'],\n",
       " [The real evils, indeed, of Emma's situation were the power of having rather too much her own way, and a disposition to think a little too well of herself; these were the disadvantages which threatened alloy to her many enjoyments.,\n",
       "  'Austen'],\n",
       " [The danger, however, was at present so unperceived, that they did not by any means rank as misfortunes with her.,\n",
       "  'Austen'],\n",
       " [Sorrow came a gentle sorrow but not at all in the shape of any disagreeable consciousness.,\n",
       "  'Austen'],\n",
       " [Miss Taylor married., 'Austen'],\n",
       " [It was Miss Taylor's loss which first brought grief., 'Austen'],\n",
       " [It was on the wedding-day of this beloved friend that Emma first sat in mournful thought of any continuance.,\n",
       "  'Austen'],\n",
       " [The wedding over, and the bride-people gone, her father and herself were left to dine together, with no prospect of a third to cheer a long evening.,\n",
       "  'Austen'],\n",
       " [Her father composed himself to sleep after dinner, as usual, and she had then only to sit and think of what she had lost.,\n",
       "  'Austen'],\n",
       " [The event had every promise of happiness for her friend., 'Austen'],\n",
       " [Mr. Weston was a man of unexceptionable character, easy fortune, suitable age, and pleasant manners; and there was some satisfaction in considering with what self-denying, generous friendship she had always wished and promoted the match; but it was a black morning's work for her.,\n",
       "  'Austen'],\n",
       " [The want of Miss Taylor would be felt every hour of every day., 'Austen'],\n",
       " [She recalled her past kindness the kindness, the affection of sixteen years how she had taught and how she had played with her from five years old how she had devoted all her powers to attach and amuse her in health and how nursed her through the various illnesses of childhood.,\n",
       "  'Austen'],\n",
       " [A large debt of gratitude was owing here; but the intercourse of the last seven years, the equal footing and perfect unreserve which had soon followed Isabella's marriage, on their being left to each other, was yet a dearer, tenderer recollection.,\n",
       "  'Austen'],\n",
       " [She had been a friend and companion such as few possessed: intelligent, well-informed, useful, gentle, knowing all the ways of the family, interested in all its concerns, and peculiarly interested in herself, in every pleasure, every scheme of hers one to whom she could speak every thought as it arose, and who had such an affection for her as could never find fault.,\n",
       "  'Austen'],\n",
       " [How was she to bear the change?, 'Austen'],\n",
       " [It was true that her friend was going only half a mile from them; but Emma was aware that great must be the difference between a Mrs. Weston, only half a mile from them, and a Miss Taylor in the house; and with all her advantages, natural and domestic, she was now in great danger of suffering from intellectual solitude.,\n",
       "  'Austen'],\n",
       " [She dearly loved her father, but he was no companion for her., 'Austen'],\n",
       " [He could not meet her in conversation, rational or playful., 'Austen'],\n",
       " [The evil of the actual disparity in their ages (and Mr. Woodhouse had not married early) was much increased by his constitution and habits; for having been a valetudinarian all his life, without activity of mind or body, he was a much older man in ways than in years; and though everywhere beloved for the friendliness of his heart and his amiable temper, his talents could not have recommended him at any time.,\n",
       "  'Austen'],\n",
       " [Her sister, though comparatively but little removed by matrimony, being settled in London, only sixteen miles off, was much beyond her daily reach; and many a long October and November evening must be struggled through at Hartfield, before Christmas brought the next visit from Isabella and her husband, and their little children, to fill the house, and give her pleasant society again.,\n",
       "  'Austen'],\n",
       " [Highbury, the large and populous village, almost amounting to a town, to which Hartfield, in spite of its separate lawn, and shrubberies, and name, did really belong, afforded her no equals.,\n",
       "  'Austen'],\n",
       " [The Woodhouses were first in consequence there., 'Austen'],\n",
       " [All looked up to them., 'Austen'],\n",
       " [She had many acquaintance in the place, for her father was universally civil, but not one among them who could be accepted in lieu of Miss Taylor for even half a day.,\n",
       "  'Austen'],\n",
       " [It was a melancholy change; and Emma could not but sigh over it, and wish for impossible things, till her father awoke, and made it necessary to be cheerful.,\n",
       "  'Austen'],\n",
       " [His spirits required support., 'Austen'],\n",
       " [He was a nervous man, easily depressed; fond of every body that he was used to, and hating to part with them; hating change of every kind.,\n",
       "  'Austen'],\n",
       " [Matrimony, as the origin of change, was always disagreeable; and he was by no means yet reconciled to his own daughter's marrying, nor could ever speak of her but with compassion, though it had been entirely a match of affection, when he was now obliged to part with Miss Taylor too; and from his habits of gentle selfishness, and of being never able to suppose that other people could feel differently from himself, he was very much disposed to think Miss Taylor had done as sad a thing for herself as for them, and would have been a great deal happier if she had spent all the rest of her life at Hartfield.,\n",
       "  'Austen'],\n",
       " [Emma smiled and chatted as cheerfully as she could, to keep him from such thoughts; but when tea came, it was impossible for him not to say exactly as he had said at dinner, \"Poor Miss Taylor!,\n",
       "  'Austen'],\n",
       " [I wish she were here again., 'Austen'],\n",
       " [What a pity it is that Mr. Weston ever thought of her!\", 'Austen'],\n",
       " [\"I cannot agree with you, papa;, 'Austen'],\n",
       " [you know I cannot., 'Austen'],\n",
       " [Mr. Weston is such a good-humoured, pleasant, excellent man, that he thoroughly deserves a good wife; and you would not have had Miss Taylor live with us for ever, and bear all my odd humours, when she might have a house of her own?,\n",
       "  'Austen'],\n",
       " [\" \"A house of her own!, 'Austen'],\n",
       " [But where is the advantage of a house of her own?, 'Austen'],\n",
       " [This is three times as large., 'Austen'],\n",
       " [And you have never any odd humours, my dear.\", 'Austen'],\n",
       " [\"How often we shall be going to see them, and they coming to see us!,\n",
       "  'Austen'],\n",
       " [We shall be always meeting!, 'Austen'],\n",
       " [_, 'Austen'],\n",
       " [We_ must begin; we must go and pay wedding visit very soon.\", 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [My dear, how am I to get so far?, 'Austen'],\n",
       " [Randalls is such a distance., 'Austen'],\n",
       " [I could not walk half so far., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\"No, papa, nobody thought of your walking., 'Austen'],\n",
       " [We must go in the carriage, to be sure.\", 'Austen'],\n",
       " [\"The carriage!, 'Austen'],\n",
       " [But James will not like to put the horses to for such a little way; and where are the poor horses to be while we are paying our visit?\",\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [They are to be put into Mr. Weston's stable, papa., 'Austen'],\n",
       " [You know we have settled all that already., 'Austen'],\n",
       " [We talked it all over with Mr. Weston last night., 'Austen'],\n",
       " [And as for James, you may be very sure he will always like going to Randalls, because of his daughter's being housemaid there.,\n",
       "  'Austen'],\n",
       " [I only doubt whether he will ever take us anywhere else., 'Austen'],\n",
       " [That was your doing, papa., 'Austen'],\n",
       " [You got Hannah that good place., 'Austen'],\n",
       " [Nobody thought of Hannah till you mentioned her James is so obliged to you!\",\n",
       "  'Austen'],\n",
       " [\"I am very glad I did think of her., 'Austen'],\n",
       " [It was very lucky, for I would not have had poor James think himself slighted upon any account; and I am sure she will make a very good servant,\n",
       "  'Austen'],\n",
       " [: she is a civil, pretty-spoken girl; I have a great opinion of her.,\n",
       "  'Austen'],\n",
       " [Whenever I see her, she always curtseys and asks me how I do, in a very pretty manner; and when you have had her here to do needlework, I observe she always turns the lock of the door the right way and never bangs it.,\n",
       "  'Austen'],\n",
       " [I am sure she will be an excellent servant; and it will be a great comfort to poor Miss Taylor to have somebody about her that she is used to see.,\n",
       "  'Austen'],\n",
       " [Whenever James goes over to see his daughter, you know, she will be hearing of us.,\n",
       "  'Austen'],\n",
       " [He will be able to tell her how we all are., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Emma spared no exertions to maintain this happier flow of ideas, and hoped, by the help of backgammon, to get her father tolerably through the evening, and be attacked by no regrets but her own.,\n",
       "  'Austen'],\n",
       " [The backgammon-table was placed; but a visitor immediately afterwards walked in and made it unnecessary.,\n",
       "  'Austen'],\n",
       " [Mr. Knightley, a sensible man about seven or eight-and-thirty, was not only a very old and intimate friend of the family, but particularly connected with it, as the elder brother of Isabella's husband.,\n",
       "  'Austen'],\n",
       " [He lived about a mile from Highbury, was a frequent visitor, and always welcome, and at this time more welcome than usual, as coming directly from their mutual connexions in London.,\n",
       "  'Austen'],\n",
       " [He had returned to a late dinner, after some days' absence, and now walked up to Hartfield to say that all were well in Brunswick Square.,\n",
       "  'Austen'],\n",
       " [It was a happy circumstance, and animated Mr. Woodhouse for some time.,\n",
       "  'Austen'],\n",
       " [Mr. Knightley had a cheerful manner, which always did him good; and his many inquiries after \"poor Isabella\" and her children were answered most satisfactorily.,\n",
       "  'Austen'],\n",
       " [When this was over, Mr. Woodhouse gratefully observed, \"It is very kind of you, Mr. Knightley, to come out at this late hour to call upon us.,\n",
       "  'Austen'],\n",
       " [I am afraid you must have had a shocking walk., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\"Not at all, sir., 'Austen'],\n",
       " [It is a beautiful moonlight night; and so mild that I must draw back from your great fire.\",\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [But you must have found it very damp and dirty., 'Austen'],\n",
       " [I wish you may not catch cold.\", 'Austen'],\n",
       " [\"Dirty, sir!, 'Austen'],\n",
       " [Look at my shoes., 'Austen'],\n",
       " [Not a speck on them., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Well! that is quite surprising, for we have had a vast deal of rain here.,\n",
       "  'Austen'],\n",
       " [It rained dreadfully hard for half an hour while we were at breakfast.,\n",
       "  'Austen'],\n",
       " [I wanted them to put off the wedding., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [By the bye I have not wished you joy., 'Austen'],\n",
       " [Being pretty well aware of what sort of joy you must both be feeling, I have been in no hurry with my congratulations; but I hope it all went off tolerably well.,\n",
       "  'Austen'],\n",
       " [How did you all behave?, 'Austen'],\n",
       " [Who cried most?\", 'Austen'],\n",
       " [\"Ah! poor Miss Taylor! ', 'Austen'],\n",
       " [Tis a sad business., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\"Poor Mr. and Miss Woodhouse, if you please; but I cannot possibly say `poor Miss Taylor.',\n",
       "  'Austen'],\n",
       " [I have a great regard for you and Emma; but when it comes to the question of dependence or independence!,\n",
       "  'Austen'],\n",
       " [At any rate, it must be better to have only one to please than two.\",\n",
       "  'Austen'],\n",
       " [\"Especially when _one_ of those two is such a fanciful, troublesome creature!\" said Emma playfully.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [That is what you have in your head, I know and what you would certainly say if my father were not by.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\"I believe it is very true, my dear, indeed,\" said Mr. Woodhouse, with a sigh.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [I am afraid I am sometimes very fanciful and troublesome.\", 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [My dearest papa!, 'Austen'],\n",
       " [You do not think I could mean, 'Austen'],\n",
       " [_you_, or suppose Mr. Knightley to mean _, 'Austen'],\n",
       " [you_., 'Austen'],\n",
       " [What a horrible idea!, 'Austen'],\n",
       " [Oh no!, 'Austen'],\n",
       " [I meant only myself., 'Austen'],\n",
       " [Mr. Knightley loves to find fault with me, you know in a joke it is all a joke.,\n",
       "  'Austen'],\n",
       " [We always say what we like to one another., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Mr. Knightley, in fact, was one of the few people who could see faults in Emma Woodhouse, and the only one who ever told her of them: and though this was not particularly agreeable to Emma herself, she knew it would be so much less so to her father, that she would not have him really suspect such a circumstance as her not being thought perfect by every body.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Emma knows I never flatter her,\" said Mr. Knightley, \"but I meant no reflection on any body.,\n",
       "  'Austen'],\n",
       " [Miss Taylor has been used to have two persons to please; she will now have but one.,\n",
       "  'Austen'],\n",
       " [The chances are that she must be a gainer.\", 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Well,\" said Emma, willing to let it pass \"you want to hear about the wedding; and I shall be happy to tell you, for we all behaved charmingly.,\n",
       "  'Austen'],\n",
       " [Every body was punctual, every body in their best looks: not a tear, and hardly a long face to be seen.,\n",
       "  'Austen'],\n",
       " [Oh no; we all felt that we were going to be only half a mile apart, and were sure of meeting every day.\",\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Dear Emma bears every thing so well,\" said her father., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [But, Mr. Knightley, she is really very sorry to lose poor Miss Taylor, and I am sure she _will_ miss her more than she thinks for.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Emma turned away her head, divided between tears and smiles., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [It is impossible that Emma should not miss such a companion,\" said Mr. Knightley.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [We should not like her so well as we do, sir, if we could suppose it; but she knows how much the marriage is to Miss Taylor's advantage; she knows how very acceptable it must be, at Miss Taylor's time of life, to be settled in a home of her own, and how important to her to be secure of a comfortable provision, and therefore cannot allow herself to feel so much pain as pleasure.,\n",
       "  'Austen'],\n",
       " [Every friend of Miss Taylor must be glad to have her so happily married.\",\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [And you have forgotten one matter of joy to me,\" said Emma, \"and a very considerable one that I made the match myself.,\n",
       "  'Austen'],\n",
       " [I made the match, you know, four years ago; and to have it take place, and be proved in the right, when so many people said Mr. Weston would never marry again, may comfort me for any thing.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Mr. Knightley shook his head at her., 'Austen'],\n",
       " [Her father fondly replied, \"Ah!, 'Austen'],\n",
       " [my dear, I wish you would not make matches and foretell things, for whatever you say always comes to pass.,\n",
       "  'Austen'],\n",
       " [Pray do not make any more matches., 'Austen'],\n",
       " [\", 'Austen'],\n",
       " [\"I promise you to make none for myself, papa; but I must, indeed, for other people.,\n",
       "  'Austen'],\n",
       " [It is the greatest amusement in the world!, 'Austen'],\n",
       " [And after such success, you know!, 'Austen'],\n",
       " [Every body said that Mr. Weston would never marry again., 'Austen'],\n",
       " [Oh dear, no!, 'Austen'],\n",
       " [Mr. Weston, who had been a widower so long, and who seemed so perfectly comfortable without a wife, so constantly occupied either in his business in town or among his friends here, always acceptable wherever he went, always cheerful Mr. Weston need not spend a single evening in the year alone if he did not like it.,\n",
       "  'Austen'],\n",
       " [Oh no!, 'Austen'],\n",
       " [Mr. Weston certainly would never marry again., 'Austen'],\n",
       " [Some people even talked of a promise to his wife on her deathbed, and others of the son and the uncle not letting him.,\n",
       "  'Austen'],\n",
       " [All manner of solemn nonsense was talked on the subject, but I believed none of it.,\n",
       "  'Austen'],\n",
       " [\", 'Austen'],\n",
       " [Ever since the day about four years ago that Miss Taylor and I met with him in Broadway Lane, when, because it began to drizzle, he darted away with so much gallantry, and borrowed two umbrellas for us from Farmer Mitchell's, I made up my mind on the,\n",
       "  'Austen']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "emma_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.7235772357723578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>158</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen      158       12\n",
       "Carroll      56       20"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n",
    "\n",
    "## Model 1. SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set acc:  0.7142857142857143\n",
      "Test set acc:  0.702247191011236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "print('Train set acc: ', model.score(X_train, y_train))\n",
    "print('Test set acc: ', model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2. Logistic Classifier (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8872180451127819\n",
      "\n",
      "Test set score: 0.8314606741573034\n",
      "Cross-validation acc: mean 0.8157931516422083, std: 0.014001375023192947\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1', C=1, solver = 'saga') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "lr.fit(X_train, y_train)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "cvs = cross_val_score(lr, X_train, y_train, cv = 5)\n",
    "print('Cross-validation acc: mean {}, std: {}'.format(np.mean(cvs), np.std(cvs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3. XGBOOST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bylevel': 0.2,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "max_depth = [3, 4, 5, 6]\n",
    "estimators = [100, 150, 200,]\n",
    "learning_rate = [0.1, 0.2, 0.3]\n",
    "colsample = [0.1, 0.2, 0.3]\n",
    "\n",
    "params = dict(max_depth = max_depth, n_estimators = estimators, \n",
    "              learning_rate = learning_rate, colsample_bylevel = colsample)\n",
    "\n",
    "gs = GridSearchCV(xgb.XGBClassifier(), params, cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8796992481203008\n",
      "\n",
      "Test set score: 0.8202247191011236\n",
      "Cross-validation acc: mean 0.8082459818308875, std: 0.022167216092698065\n"
     ]
    }
   ],
   "source": [
    "xgb = xgb.XGBClassifier(**gs.best_params_)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('Training set score:', xgb.score(X_train, y_train))\n",
    "print('\\nTest set score:', xgb.score(X_test, y_test))\n",
    "cvs = cross_val_score(xgb, X_train, y_train, cv = 5)\n",
    "print('Cross-validation acc: mean {}, std: {}'.format(np.mean(cvs), np.std(cvs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw:\n",
      " [Paradise Lost by John Milton 1667] \n",
      " \n",
      " \n",
      "Book I \n",
      " \n",
      " \n",
      "Of Man's first disobedience, and the fruit \n",
      "Of \n"
     ]
    }
   ],
   "source": [
    "# load the book\n",
    "paradise = gutenberg.raw('milton-paradise.txt')\n",
    "print('\\nRaw:\\n', paradise[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paradise Lost by John Milton 1667] \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Of Man's first disobedience, and the fruit \n",
      "Of that fo\n"
     ]
    }
   ],
   "source": [
    "# clean the chapter\n",
    "paradise = re.sub(r'Book .*', \"\", paradise)\n",
    "print(paradise[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n"
     ]
    }
   ],
   "source": [
    "# clean the novel\n",
    "paradise = text_cleaner(paradise[:int(len(paradise)/10)])\n",
    "\n",
    "# parse the cleaned novel\n",
    "paradise_doc = nlp(paradise)\n",
    "\n",
    "# group into sentences\n",
    "paradise_sents = [[sent, 'Milton'] for sent in paradise_doc.sents]\n",
    "\n",
    "# equalize with the lenght of Alice\n",
    "paradise_sents = paradise_sents[:len(alice_sents)]\n",
    "\n",
    "# create a dataframe\n",
    "paradise_sents = pd.DataFrame(paradise_sents)\n",
    "\n",
    "# the same common words from Alice and Persuasion.\n",
    "paradise_bow = bow_features(paradise_sents, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test sets for Paradise\n",
    "par_X_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    paradise_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "par_y_test = pd.concat([y_train[y_train == 'Carroll'], pd.Series(['Milton']*paradise_bow.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.9512195121951219\n",
      "Cross-validation acc: mean 0.7610278745644599, std: 0.046737832274326936\n"
     ]
    }
   ],
   "source": [
    "# logistic classifier\n",
    "lr = LogisticRegression()\n",
    "lr.fit(par_X_test, par_y_test)\n",
    "print('\\nTest set score:', lr.score(par_X_test, par_y_test))\n",
    "cvs = cross_val_score(lr, par_X_test, par_y_test, cv = 5)\n",
    "print('Cross-validation acc: mean {}, std: {}'.format(np.mean(cvs), np.std(cvs)))\n",
    "y_pred = lr.predict(par_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Carroll       1.00      0.87      0.93        76\n",
      "     Milton       0.93      1.00      0.96       129\n",
      "\n",
      "avg / total       0.95      0.95      0.95       205\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix and classification report\n",
    "print(classification_report(par_y_test,y_pred))\n",
    "sns.heatmap(confusion_matrix(par_y_test,y_pred),annot=True,fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "SVC model with default parameters did not improve the accuracy. However, Logistic classifier with Lasso regularization gave the best results even compared to the results obtained from XGBoost classifier. \n",
    "I tested the model to see if model is able to identify Alice in Wonderland versus Paradise novel by Milton. From the report we can see that the model is able to classify Milton's Paradise better than Alice in wonderland with recall score 1. and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
